import sqlite3
import chromadb
from chromadb.utils import embedding_functions
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

DB_PATH = os.getenv("DATABASE_PATH", "backend/crawler/forums.db")
CHROMA_PATH = os.getenv("CHROMA_PATH", "data/chroma_db")

def setup_chroma():
    print(f"Initializing ChromaDB at {CHROMA_PATH}...")
    if not os.path.exists(CHROMA_PATH):
        os.makedirs(CHROMA_PATH)
        
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    
    # Use a high quality, free local model
    # all-MiniLM-L6-v2 is the default for Chroma but explicit is better
    ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="all-MiniLM-L6-v2"
    )
    
    collection = client.get_or_create_collection(
        name="avid_posts",
        embedding_function=ef,
        metadata={"hnsw:space": "cosine"}
    )
    return collection

def fetch_threads_from_sqlite():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    # Fetch original question content from threads
    c.execute('''
        SELECT id, question_content as content, 'System' as author, scraped_at as post_date, title, url 
        FROM threads
    ''')
    threads = c.fetchall()
    conn.close()
    return threads

def ingest_vectors():
    """å‘é‡åŒ–è®ºå›å¸–å­ï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰"""
    collection = setup_chroma()
    threads = fetch_threads_from_sqlite()

    print(f"Found {len(threads)} threads to ingest.")

    ids = []
    documents = []
    metadatas = []

    for thread in threads:
        content = thread['content']
        if not content or len(content.strip()) < 10:
            continue

        # Combine title + content for better semantic search
        full_text = f"Title: {thread['title']}\nContent: {content}"

        # Use thread ID (which is the URL) for IDs
        import hashlib
        short_id = hashlib.md5(thread['id'].encode()).hexdigest()
        ids.append(f"thread_{short_id}")

        documents.append(full_text)
        metadatas.append({
            "source": "forum",
            "url": thread['url'],
            "author": thread['author'],
            "date": thread['post_date'],
            "title": thread['title']
        })

        # Batch ingest every 100 items
        if len(ids) >= 100:
            print(f"Upserting batch of {len(ids)}...")
            collection.upsert(ids=ids, documents=documents, metadatas=metadatas)
            ids = []
            documents = []
            metadatas = []

    # Final batch
    if ids:
        print(f"Upserting final batch of {len(ids)}...")
        collection.upsert(ids=ids, documents=documents, metadatas=metadatas)

    print(f"Forum ingestion complete. Total items in collection: {collection.count()}")


def ingest_pdf_chunks(pdf_id: int, log_callback=None):
    """
    å‘é‡åŒ–å•ä¸ª PDF æ–‡æ¡£ï¼ˆæµå¼å¤„ç†ä¼˜åŒ–ç‰ˆ - é€‚ç”¨äºå¤§æ–‡æ¡£ï¼‰

    Args:
        pdf_id: PDF åœ¨æ•°æ®åº“ä¸­çš„ ID
        log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
    """
    def log(msg):
        if log_callback:
            log_callback(msg)
        else:
            print(msg)

    try:
        from pdf_extractor_large import LargePDFExtractor
        from pdf_schema import get_pdf_by_id, update_pdf_status

        # è·å– PDF ä¿¡æ¯
        pdf_record = get_pdf_by_id(pdf_id)
        if not pdf_record:
            log(f"âŒ PDF ID {pdf_id} not found in database")
            return False

        log(f"ğŸ“„ Processing: {pdf_record['filename']}")

        # åˆ›å»ºæå–å™¨
        extractor = LargePDFExtractor(pdf_record['file_path'])

        # å…ˆè·å–PDFåŸºæœ¬ä¿¡æ¯
        pdf_info = extractor.get_pdf_info()
        if not pdf_info:
            log(f"âš ï¸ Failed to read PDF info")
            return False

        total_pages = pdf_info['total_pages']
        file_size = pdf_info['file_size']
        log(f"  ğŸ“Š File size: {file_size / 1024 / 1024:.2f} MB")
        log(f"  ğŸ“Š Total pages: {total_pages}")

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“å’Œè®¡æ•°å™¨
        collection = setup_chroma()
        total_chunks = 0
        batch_size = 100  # å¢åŠ æ‰¹æ¬¡å¤§å°åˆ°100ï¼Œå‡å°‘æ•°æ®åº“IOPSå’ŒWALæ–‡ä»¶å¢é•¿
        start_time = None  # ç”¨äºè®¡ç®—é€Ÿåº¦
        vectorizing_start_page = 0  # è®°å½•å‘é‡åŒ–èµ·å§‹é¡µ

        import gc


        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(current_page: int, total_pg: int, message: str):
            """æµå¼å¤„ç†è¿›åº¦å›è°ƒ - å‘é€ç»“æ„åŒ–è¿›åº¦æ•°æ®"""
            import time

            nonlocal start_time
            if start_time is None:
                start_time = time.time()

            # è®¡ç®—å¤„ç†é€Ÿåº¦å’Œé¢„ä¼°æ—¶é—´
            elapsed = time.time() - start_time
            speed = current_page / elapsed if elapsed > 0 else 0
            eta = (total_pg - current_page) / speed if speed > 0 else 0

            # å‘é€æ–‡æœ¬æ—¥å¿—ï¼ˆå…¼å®¹æ€§ï¼‰
            log(f"  â³ {message} (é€Ÿåº¦: {speed:.1f}é¡µ/ç§’, å‰©ä½™: {eta:.0f}ç§’)")

            # å‘é€ç»“æ„åŒ–è¿›åº¦æ•°æ®ï¼ˆæ–°å¢ï¼‰
            if log_callback:
                progress_data = {
                    "type": "progress",
                    "current": current_page,
                    "total": total_pg,
                    "chunks": total_chunks,
                    "speed": round(speed, 2),  # ä¿ç•™2ä½å°æ•°æ›´å‡†ç¡®
                    "percentage": round((current_page / total_pg * 100), 1) if total_pg > 0 else 0,
                    "eta": round(eta)  # é¢„è®¡å‰©ä½™æ—¶é—´ï¼ˆç§’ï¼‰
                }
                log_callback(progress_data, type="progress")

        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        update_pdf_status(pdf_id, 'processing', total_pages=total_pages)

        log(f"  ğŸ”„ Starting text extraction (streaming mode)...")

        # æµå¼å¤„ç†ï¼šè¾¹æå–è¾¹å‘é‡åŒ–ï¼Œå†…å­˜ä¼˜åŒ–
        try:
            failed_batches = []  # è®°å½•å¤±è´¥çš„æ‰¹æ¬¡
            retry_count = 0
            max_retries = 2  # æœ€å¤šé‡è¯•2æ¬¡

            for batch in extractor.extract_text_stream(
                batch_size=batch_size,
                progress_callback=progress_callback
            ):
                if not batch:
                    continue

                # è·å–è¿™æ‰¹æ•°æ®çš„é¡µç èŒƒå›´
                first_page = batch[0]['metadata']['page']
                last_page = batch[-1]['metadata']['page']

                # å‡†å¤‡è¿™æ‰¹æ•°æ®
                ids = [chunk['id'] for chunk in batch]
                documents = [chunk['content'] for chunk in batch]
                metadatas = [chunk['metadata'] for chunk in batch]

                try:
                    import time
                    vector_start = time.time()

                    # å‘é‡åŒ–å‰æç¤º
                    log(f"  ğŸ” Vectorizing {len(batch)} chunks from pages {first_page}-{last_page}...")

                    # ç›´æ¥æ‰§è¡Œ upsert
                    collection.upsert(ids=ids, documents=documents, metadatas=metadatas)

                    vector_time = time.time() - vector_start
                    if vector_time > 30:
                        log(f"  âš ï¸ Vectorizing took {vector_time:.1f}s (slow batch)")

                    # æ›´æ–°è®¡æ•°
                    total_chunks += len(batch)

                    # å‘é‡åŒ–å®Œæˆæç¤º
                    log(f"  âœ… Vectorized {len(batch)} chunks (pages {first_page}-{last_page})")

                    # æ˜¾å¼è§¦å‘GCï¼Œé˜²æ­¢å¤§å¯¹è±¡å †ç§¯
                    if total_chunks % 500 == 0:
                        gc.collect()

                except Exception as batch_error:
                    log(f"  âŒ Failed to vectorize batch {first_page}-{last_page}: {str(batch_error)[:100]}")
                    failed_batches.append((first_page, last_page, str(batch_error)[:100]))

                # å‘é€è¿›åº¦æ›´æ–°ï¼ˆä½¿ç”¨æœ€åå¤„ç†çš„é¡µç ï¼‰
                progress_callback(last_page, total_pages, f"Processed {last_page}/{total_pages} pages")

            # æ‰“å°å¤„ç†æ€»ç»“
            if failed_batches:
                log(f"âš ï¸ Processing completed with {len(failed_batches)} failed batches:")
                for first, last, error in failed_batches[:5]:
                    log(f"   - Pages {first}-{last}: {error}")
                if len(failed_batches) > 5:
                    log(f"   ... and {len(failed_batches) - 5} more")
            else:
                log(f"âœ… All batches processed successfully")

        except Exception as stream_error:
            log(f"âŒ Stream processing error: {stream_error}")
            update_pdf_status(pdf_id, 'failed', error_msg=str(stream_error))
            return False

        # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆï¼ˆæˆ–éƒ¨åˆ†å®Œæˆï¼‰
        status = 'completed' if not failed_batches else 'partial'
        update_pdf_status(pdf_id, status,
                         total_pages=total_pages,
                         total_chunks=total_chunks)

        log(f"âœ… PDF {pdf_record['filename']} indexing complete!")
        log(f"   ğŸ“ˆ Total: {total_pages} pages, {total_chunks} chunks")
        return True

    except Exception as e:
        import traceback
        error_msg = str(e)
        log(f"âŒ Error ingesting PDF: {error_msg}")
        log(traceback.format_exc())

        if pdf_id:
            from pdf_schema import update_pdf_status
            update_pdf_status(pdf_id, 'failed', error_msg=error_msg)

        return False


def delete_pdf_from_chroma(pdf_id: int):
    """ä» ChromaDB åˆ é™¤ PDF çš„æ‰€æœ‰å‘é‡"""
    try:
        from pdf_schema import get_pdf_by_id

        pdf_record = get_pdf_by_id(pdf_id)
        if not pdf_record:
            return False

        collection = setup_chroma()

        # æŸ¥è¯¢è¯¥ PDF çš„æ‰€æœ‰ chunk ID
        # ChromaDB ä¸æ”¯æŒç›´æ¥çš„ where deleteï¼Œéœ€è¦å…ˆæŸ¥è¯¢
        results = collection.get(
            where={"filename": pdf_record['filename']}
        )

        if results and results.get('ids'):
            collection.delete(ids=results['ids'])
            print(f"Deleted {len(results['ids'])} chunks from ChromaDB")
            return True

        return False

    except Exception as e:
        print(f"Error deleting from ChromaDB: {e}")
        return False

if __name__ == "__main__":
    ingest_vectors()
